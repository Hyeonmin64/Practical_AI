[How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding](https://arxiv.org/abs/2303.04245) (1)

-
-

[Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846) (2)

-
-

[STABILIZING TRANSFORMER TRAINING BY PREVENTING ATTENTION ENTROPY COLLAPSE](https://arxiv.org/pdf/2303.06296.pdf) (3)

-
-

[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://nips.cc/virtual/2022/poster/54470)

-
-

[Data Distributional Properties Drive Emergent In-Context Learning in Transformers](https://nips.cc/virtual/2022/poster/54758)

-
-

[What Can Transformers Learn In-Context? A Case Study of Simple Function Classes
](https://nips.cc/virtual/2022/poster/53586) (4)

-
-

[What learning algorithm is in-context learning? Investigations with linear models](https://openreview.net/forum?id=0g0X4H8yN4I) (5)

-
-

[A Survey on In-context Learning](https://arxiv.org/abs/2301.00234)

-
-

[PREFERENCE TRANSFORMER: MODELING HUMAN
PREFERENCES USING TRANSFORMERS FOR RL](https://openreview.net/pdf?id=Peot1SFDX0)

-
-


Question) in-context learning?

Answer) [How does in-context learning work? A framework for understanding the differences from traditional supervised learning](http://ai.stanford.edu/blog/understanding-incontext/)
