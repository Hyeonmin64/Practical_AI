[How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding](https://arxiv.org/abs/2303.04245)

-
-

[Larger language models do in-context learning differently](https://arxiv.org/abs/2303.03846)

-
-

[STABILIZING TRANSFORMER TRAINING BY PREVENTING ATTENTION ENTROPY COLLAPSE](https://arxiv.org/pdf/2303.06296.pdf)

-
-

[Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://nips.cc/virtual/2022/poster/54470)

-
-

[Data Distributional Properties Drive Emergent In-Context Learning in Transformers](https://nips.cc/virtual/2022/poster/54758)

-
-

[What Can Transformers Learn In-Context? A Case Study of Simple Function Classes
](https://nips.cc/virtual/2022/poster/53586)

-
-

[What learning algorithm is in-context learning? Investigations with linear models](https://openreview.net/forum?id=0g0X4H8yN4I)

-
-


Question) in-context learning?

Answer) [How does in-context learning work? A framework for understanding the differences from traditional supervised learning](http://ai.stanford.edu/blog/understanding-incontext/)
